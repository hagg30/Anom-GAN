{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade torch torchvision\n",
    "#!/notebooks/py_36_env/bin/python train.py --mixing --loss r1 --sched lmdb_out\n",
    "#!pip3 install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir checkpoint/capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir sample/capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip ./good.zip -d ./mvtec_single_dataset/capsule\n",
    "#!mkdir mvtec_single_out/capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/notebooks/py_36_env/bin/python prepare_data.py --out mvtec_single_out/capsule --n_worker 4 mvtec_single_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/0_KU-KAIST/style-based-gan-pytorch/style-based-gan-pytorch'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: future_fstrings     # should work even without -*-\n",
    "#location = fromstr(f'POINT({longitude} {latitude})', srid=4326)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import sys\n",
    "sys.argv = [sys.argv[0], '--sched', 'mvtec_single_out/capsule']# '--loss', 'r1', '--mixing'\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from dataset import MultiResolutionDataset\n",
    "from model import StyledGenerator, Discriminator, Encoder\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "def sample_data(dataset, batch_size, image_size=4):\n",
    "    dataset.resolution = image_size\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=1, drop_last=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def adjust_lr(optimizer, lr):\n",
    "    for group in optimizer.param_groups:\n",
    "        mult = group.get('mult', 1)\n",
    "        group['lr'] = lr * mult\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    }
   ],
   "source": [
    "code_size = 512\n",
    "base_batch_size = 1 #16\n",
    "n_critic = 1\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Progressive Growing of GANs')\n",
    "\n",
    "parser.add_argument('path', type=str, help='path of specified dataset')\n",
    "parser.add_argument(\n",
    "    '--phase',\n",
    "    type=int,\n",
    "    default= 30000, #600_000 // 4,\n",
    "    help='number of samples used for each training phases',\n",
    ")\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--sched', action='store_true', help='use lr scheduling')\n",
    "parser.add_argument('--init_size', default=8, type=int, help='initial image size') #8\n",
    "parser.add_argument('--max_size', default=512, type=int, help='max image size')\n",
    "parser.add_argument(\n",
    "    '--ckpt', default=None, type=str, help='load from previous checkpoints'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no_from_rgb_activate',\n",
    "    action='store_true',\n",
    "    help='use activate in from_rgb (original implementation)',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mixing', action='store_true', help='use mixing regularization'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--loss',\n",
    "    type=str,\n",
    "    default='wgan-gp',\n",
    "    choices=['wgan-gp', 'r1'],\n",
    "    help='class of gan loss',\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "encoder = nn.DataParallel(\n",
    "    Encoder(from_rgb_activate=not args.no_from_rgb_activate)\n",
    ").cuda()\n",
    "generator = nn.DataParallel(StyledGenerator(code_size)).cuda()\n",
    "discriminator = nn.DataParallel(\n",
    "    Discriminator(from_rgb_activate=not args.no_from_rgb_activate)\n",
    ").cuda()\n",
    "g_running = StyledGenerator(code_size).cuda()\n",
    "g_running.train(False)\n",
    "\n",
    "e_running = Encoder(from_rgb_activate=not args.no_from_rgb_activate).cuda()\n",
    "e_running.train(False)\n",
    "\n",
    "g_optimizer = optim.Adam(\n",
    "    generator.module.generator.parameters(), lr=args.lr, betas=(0.0, 0.99)\n",
    ")\n",
    "g_optimizer.add_param_group(\n",
    "    {\n",
    "        'params': generator.module.style.parameters(),\n",
    "        'lr': args.lr * 0.01,\n",
    "        'mult': 0.01,\n",
    "    }\n",
    ")\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "#beta1=0.9, beta2=0.99, epsilon=1e-8)\n",
    "e_optimizer = optim.Adam(encoder.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "\n",
    "accumulate(g_running, generator.module, 0)\n",
    "accumulate(e_running, encoder.module, 0)\n",
    "\n",
    "#assert args.ckpt is not None\n",
    "if args.ckpt is not None:\n",
    "    ckpt = torch.load(args.ckpt)\n",
    "\n",
    "    generator.module.load_state_dict(ckpt['generator'])\n",
    "    discriminator.module.load_state_dict(ckpt['discriminator'])\n",
    "    g_running.load_state_dict(ckpt['g_running'])\n",
    "    g_optimizer.load_state_dict(ckpt['g_optimizer'])\n",
    "    d_optimizer.load_state_dict(ckpt['d_optimizer'])\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = MultiResolutionDataset(args.path, transform)\n",
    "\n",
    "if args.sched:\n",
    "    args.lr = {128: 0.0015, 256: 0.002, 512: 0.002, 1024: 0.002}\n",
    "    #args.batch = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 32, 256: 32}\n",
    "    args.batch = {4: base_batch_size*64, 8: base_batch_size*32, 16: base_batch_size*32, 32: base_batch_size*16,\n",
    "                  64: base_batch_size*4, 128: base_batch_size*2, 256: base_batch_size, 512: base_batch_size}\n",
    "\n",
    "else:\n",
    "    args.lr = {}\n",
    "    args.batch = {}\n",
    "\n",
    "args.gen_sample = {512: (8, 4), 1024: (4, 2)}\n",
    "\n",
    "args.batch_default = base_batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_g = []\n",
    "loss_e = []\n",
    "loss_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Size: 512; E: 3.434; G: 9.068; D: -2.280; Grad: 0.003; Alpha: 1.00000:  12%|█▏        | 354287/3000000 [76:17:55<569:46:40,  1.29it/s]                     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-81fa1be03c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m#gen_in2 = encoder(real_image, step=step, alpha=alpha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_in2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wgan-gp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/0_KU-KAIST/style-based-gan-pytorch/style-based-gan-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, step, alpha)\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_std\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogression\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/0_KU-KAIST/style-based-gan-pytorch/style-based-gan-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/0_KU-KAIST/style-based-gan-pytorch/style-based-gan-pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    106\u001b[0m         ) / 4\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train(args, dataset, generator, discriminator, 0)\n",
    "\n",
    "\n",
    "#def train(args, dataset, generator, discriminator, initial_num):\n",
    "step = int(math.log2(args.init_size)) - 2\n",
    "resolution = 4 * 2 ** step\n",
    "loader = sample_data(\n",
    "    dataset, args.batch.get(resolution, args.batch_default), resolution\n",
    ")\n",
    "data_loader = iter(loader)\n",
    "\n",
    "adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "\n",
    "pbar = tqdm(range(3_000_000)) #3_000_000\n",
    "\n",
    "requires_grad(generator, False)\n",
    "requires_grad(discriminator, True)\n",
    "\n",
    "disc_loss_val = 0\n",
    "gen_loss_val = 0\n",
    "grad_loss_val = 0\n",
    "\n",
    "alpha = 0\n",
    "used_sample = 0\n",
    "\n",
    "max_step = int(math.log2(args.max_size)) - 2\n",
    "final_progress = False\n",
    "\n",
    "for i in pbar:\n",
    "    discriminator.zero_grad()\n",
    "\n",
    "    alpha = min(1, 1 / args.phase * (used_sample + 1))\n",
    "\n",
    "    if (resolution == args.init_size and args.ckpt is None) or final_progress:\n",
    "        alpha = 1\n",
    "\n",
    "    if used_sample > args.phase * 2:\n",
    "        used_sample = 0\n",
    "        step += 1\n",
    "\n",
    "        if step > max_step:\n",
    "            step = max_step\n",
    "            final_progress = True\n",
    "            ckpt_step = step + 1\n",
    "\n",
    "        else:\n",
    "            alpha = 0\n",
    "            ckpt_step = step\n",
    "\n",
    "        resolution = 4 * 2 ** step\n",
    "\n",
    "        del loader\n",
    "        del data_loader \n",
    "\n",
    "        loader = sample_data(\n",
    "            dataset, args.batch.get(resolution, args.batch_default), resolution\n",
    "        )\n",
    "        data_loader = iter(loader)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'encoder': encoder.module.state_dict(),\n",
    "                'generator': generator.module.state_dict(),\n",
    "                'discriminator': discriminator.module.state_dict(),\n",
    "                'g_optimizer': g_optimizer.state_dict(),\n",
    "                'd_optimizer': d_optimizer.state_dict(),\n",
    "                'g_running': g_running.state_dict(),\n",
    "                'e_running': e_running.state_dict(),\n",
    "            },\n",
    "            ('checkpoint/capsule/train_step-' + str(ckpt_step) + '.model'),\n",
    "        )\n",
    "\n",
    "        adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "        adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "\n",
    "    try:\n",
    "        real_image = next(data_loader)\n",
    "\n",
    "    except (OSError, StopIteration):\n",
    "        data_loader = iter(loader)\n",
    "        real_image = next(data_loader)\n",
    "\n",
    "    used_sample += real_image.shape[0]\n",
    "\n",
    "    b_size = real_image.size(0)\n",
    "    real_image = real_image.cuda()\n",
    "\n",
    "    if args.loss == 'wgan-gp':\n",
    "        real_predict = discriminator(real_image, step=step, alpha=alpha)\n",
    "        real_predict = real_predict.mean() - 0.001 * (real_predict ** 2).mean()\n",
    "        (-real_predict).backward()\n",
    "\n",
    "    elif args.loss == 'r1':\n",
    "        real_image.requires_grad = True\n",
    "        real_scores = discriminator(real_image, step=step, alpha=alpha)\n",
    "        real_predict = F.softplus(-real_scores).mean()\n",
    "        real_predict.backward(retain_graph=True)\n",
    "\n",
    "        grad_real = grad(\n",
    "            outputs=real_scores.sum(), inputs=real_image, create_graph=True\n",
    "        )[0]\n",
    "        grad_penalty = (\n",
    "            grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "        ).mean()\n",
    "        grad_penalty = 10 / 2 * grad_penalty\n",
    "        grad_penalty.backward()\n",
    "        if i%10 == 0:\n",
    "            grad_loss_val = grad_penalty.item()\n",
    "\n",
    "    if args.mixing and random.random() < 0.9:\n",
    "        gen_in11, gen_in12, gen_in21, gen_in22 = torch.randn(\n",
    "            4, b_size, code_size, device='cuda'\n",
    "        ).chunk(4, 0)\n",
    "        gen_in1 = [gen_in11.squeeze(0), gen_in12.squeeze(0)]\n",
    "        gen_in2 = [gen_in21.squeeze(0), gen_in22.squeeze(0)]\n",
    "\n",
    "    else:\n",
    "        gen_in1, gen_in2 = torch.randn(2, b_size, code_size, device='cuda').chunk(\n",
    "            2, 0\n",
    "        )\n",
    "        gen_in1 = gen_in1.squeeze(0)\n",
    "        gen_in2 = gen_in2.squeeze(0)\n",
    "\n",
    "    #gen_in1 = encoder(real_image, step=step, alpha=alpha)\n",
    "\n",
    "    _, fake_image = generator([gen_in1], step=step, alpha=alpha)\n",
    "    fake_predict = discriminator(fake_image, step=step, alpha=alpha)\n",
    "\n",
    "    if args.loss == 'wgan-gp':\n",
    "        fake_predict = fake_predict.mean()\n",
    "        fake_predict.backward()\n",
    "\n",
    "        eps = torch.rand(b_size, 1, 1, 1).cuda()\n",
    "        x_hat = eps * real_image.data + (1 - eps) * fake_image.data\n",
    "        x_hat.requires_grad = True\n",
    "        hat_predict = discriminator(x_hat, step=step, alpha=alpha)\n",
    "        grad_x_hat = grad(\n",
    "            outputs=hat_predict.sum(), inputs=x_hat, create_graph=True\n",
    "        )[0]\n",
    "        grad_penalty = (\n",
    "            (grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2\n",
    "        ).mean()\n",
    "        grad_penalty = 10 * grad_penalty\n",
    "        grad_penalty.backward()\n",
    "        if i%10 == 0:\n",
    "            grad_loss_val = grad_penalty.item()\n",
    "            disc_loss_val = (-real_predict + fake_predict).item()\n",
    "            loss_d.append(disc_loss_val)\n",
    "\n",
    "    elif args.loss == 'r1':\n",
    "        fake_predict = F.softplus(fake_predict).mean()\n",
    "        fake_predict.backward()\n",
    "        if i%10 == 0:\n",
    "            disc_loss_val = (real_predict + fake_predict).item()\n",
    "            loss_d.append(disc_loss_val)\n",
    "\n",
    "    d_optimizer.step()\n",
    "\n",
    "\n",
    "    # Encoder \n",
    "    #generator.zero_grad()\n",
    "    encoder.zero_grad()\n",
    "\n",
    "    requires_grad(generator, True)\n",
    "    requires_grad(encoder, True)\n",
    "    requires_grad(discriminator, True)\n",
    "\n",
    "    out = encoder(real_image, step=step, alpha=alpha)\n",
    "    #fake_image = generator(feature_real, step=step, alpha=alpha)\\\n",
    "    out_orin, fake_image     = generator([gen_in1], styles=[out], step=step, alpha=alpha)\n",
    "    fake_predict = discriminator(fake_image, step=step, alpha=alpha)\n",
    "    \n",
    "    corr = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    \n",
    "    recon_loss_info = 0\n",
    "    for k in range(len(out_orin)):\n",
    "        recon_loss_info = torch.mean(corr(out,out_orin[0]))\n",
    "    #if len(out_orin) > 0:\n",
    "    #    recon_loss_info = recon_loss_info / len(out_orin)\n",
    "    \n",
    "    recon_loss_info = (1-recon_loss_info)\n",
    "\n",
    "    fake_image_norm = fake_image\n",
    "    #fake_image_norm = (fake_image_norm - torch.min(fake_image_norm))\n",
    "    #fake_image_norm = fake_image_norm / torch.max(fake_image_norm)\n",
    "\n",
    "    anomaly_image_norm = real_image\n",
    "    anomaly_image_norm = (anomaly_image_norm - torch.min(anomaly_image_norm))\n",
    "    anomaly_image_norm = anomaly_image_norm / (torch.max(anomaly_image_norm) + 1e-6)\n",
    "\n",
    "    #diff_norm = fake_image_norm - anomaly_image_norm\n",
    "    #diff_norm = torch.sum( torch.abs(diff_norm), axis = 0)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    recon_loss = loss(fake_image_norm, anomaly_image_norm)\n",
    "\n",
    "    #feature_fake = encoder(fake_image, step=step, alpha=alpha)\n",
    "    #recon_loss_feats = 0.00005 * torch.mean(torch.sum(torch.square(feature_real - feature_fake), -1))\n",
    "    #recon_loss = torch.mean(torch.sum(torch.abs(fake_image - real_image), -1))\n",
    "    #recon_loss = torch.mean(diff_norm)\n",
    "\n",
    "    #fake_image = generator(feature_real, step=step, alpha=alpha)\n",
    "    predict = discriminator(fake_image, step=step, alpha=alpha)\n",
    "\n",
    "    if args.loss == 'wgan-gp':\n",
    "        adv_loss = 0.1 * -predict.mean()\n",
    "    elif args.loss == 'r1':\n",
    "        adv_loss = 0.1 * F.softplus(-predict).mean()\n",
    "\n",
    "    loss = recon_loss + adv_loss + recon_loss_info\n",
    "\n",
    "    if i%10 == 0:\n",
    "        encoder_loss_val = loss.item()\n",
    "        loss_e.append(encoder_loss_val)\n",
    "\n",
    "    loss.backward()\n",
    "    e_optimizer.step()\n",
    "    accumulate(e_running, encoder.module)\n",
    "    #g_optimizer.step()\n",
    "    #accumulate(g_running, generator.module)\n",
    "\n",
    "    requires_grad(generator, False)\n",
    "    requires_grad(encoder, False)\n",
    "    requires_grad(discriminator, False)\n",
    "\n",
    "    # Encoder End\n",
    "\n",
    "    if (i + 1) % n_critic == 0:\n",
    "        generator.zero_grad()\n",
    "\n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator, False)\n",
    "\n",
    "        #gen_in2 = encoder(real_image, step=step, alpha=alpha)\n",
    "        _, fake_image = generator([gen_in2], step=step, alpha=alpha)\n",
    "        predict = discriminator(fake_image, step=step, alpha=alpha)\n",
    "\n",
    "        if args.loss == 'wgan-gp':\n",
    "            loss = -predict.mean()\n",
    "        elif args.loss == 'r1':\n",
    "            loss = F.softplus(-predict).mean()\n",
    "        if i%10 == 0:\n",
    "            gen_loss_val = loss.item()\n",
    "            loss_g.append(gen_loss_val)\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "        accumulate(g_running, generator.module)\n",
    "\n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator, True)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        images = []\n",
    "\n",
    "        gen_i, gen_j = args.gen_sample.get(resolution, (10, 5))\n",
    "\n",
    "        images.append(real_image.data.detach().cpu())\n",
    "        images.append(g_running(\n",
    "                        [out], styles=[out], step=step, alpha=alpha\n",
    "                    )[1].data.detach().cpu())\n",
    "                      \n",
    "        with torch.no_grad():\n",
    "            for _ in range(gen_i):\n",
    "                images.append(\n",
    "                    g_running(\n",
    "                        [torch.randn(gen_j, code_size).cuda()], step=step, alpha=alpha\n",
    "                    )[1].data.detach().cpu()\n",
    "                )\n",
    "\n",
    "        utils.save_image(\n",
    "            torch.cat(images, 0),\n",
    "            'sample/capsule/'+str(i + 1).zfill(6)+ '.png',\n",
    "            nrow=gen_i,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n",
    "\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        torch.save(\n",
    "            g_running.state_dict(), 'checkpoint/capsule/'+str(i + 1).zfill(6)+'.model'\n",
    "        )\n",
    "        torch.save(\n",
    "            e_running.state_dict(), 'checkpoint/capsule/'+str(i + 1).zfill(6)+'enc.model'\n",
    "        )\n",
    "\n",
    "    state_msg = (\n",
    "        f'Size: {4 * 2 ** step}; E: {encoder_loss_val:.3f}; G: {gen_loss_val:.3f}; D: {disc_loss_val:.3f};'\n",
    "        f' Grad: {grad_loss_val:.3f}; Alpha: {alpha:.5f}'\n",
    "    )\n",
    "\n",
    "    pbar.set_description(state_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(corr(out,out_orin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_orin[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_orin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'checkpoint/capsule/'+str(90000).zfill(6)+'enc.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "        {\n",
    "            'encoder': encoder.module.state_dict(),\n",
    "            'generator': generator.module.state_dict(),\n",
    "            'discriminator': discriminator.module.state_dict(),\n",
    "            'g_optimizer': g_optimizer.state_dict(),\n",
    "            'd_optimizer': d_optimizer.state_dict(),\n",
    "            'g_running': g_running.state_dict(),\n",
    "            'e_running': e_running.state_dict(),\n",
    "        },\n",
    "        ('checkpoint/capsule/train_step-' + str(ckpt_step) + '.model'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'checkpoint/capsule/train_step-' + str(ckpt_step) + '.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch.get(512, args.batch_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "sibal = 0\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            sibal += 1\n",
    "    except:\n",
    "        pass\n",
    "print(sibal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_i, gen_j = args.gen_sample.get(16, (10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 4\n",
    "\n",
    "gen_i, gen_j = args.gen_sample.get(16, (10, 5))\n",
    "alpha = 0.386135\n",
    "with torch.no_grad():\n",
    "    i = g_running(\n",
    "                torch.randn(gen_j, code_size).cuda(), step=2, alpha=alpha\n",
    "            ).data.cpu()\n",
    "        #)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "training_data = (np.transpose(i[0], (1,2,0)) - torch.min(i[0]) ) / ( torch.max(i[0]) - torch.min(i[0]) )\n",
    "plt.imshow( training_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(math.log2(args.init_size)) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = int(math.log2(args.max_size)) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(math.log2(args.max_size)) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
